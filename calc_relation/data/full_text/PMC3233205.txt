 BAYESIAN HIERARCHICAL MODELING FOR SIGNALING PATHWAY INFERENCE FROM SINGLE CELL INTERVENTIONAL DATA1. Recent technological advances have made it possible to simultaneously measure multiple protein activities at the single cell level. With such data collected under different stimulatory or inhibitory conditions, it is possible to infer the causal relationships among proteins from single cell interventional data. In this article we propose a Bayesian hierarchical modeling framework to infer the signaling pathway based on the posterior distributions of parameters in the model. Under this framework, we consider network sparsity and model the existence of an association between two proteins both at the overall level across all experiments and at each individual experimental level. This allows us to infer the pairs of proteins that are associated with each other and their causal relationships. We also explicitly consider both intrinsic noise and measurement error. Markov chain Monte Carlo is implemented for statistical inference. We demonstrate that this hierarchical modeling can effectively pool information from different interventional experiments through simulation studies and real data analysis. 1. Introduction. Cells respond to internal and external changes through signaling networks. One major research area in biology is to identify signaling proteins and understand how they coordinate to function properly. With recent technological advances in genomics and proteomics, researchers now can monitor and quantify molecular activities at the genome level, making it possible to reconstruct signaling pathways from these high-throughput data. Although efforts have been made to use microarray gene expression data and sequence data to reveal signaling pathways [e.g., ], these data are limited in two important aspects. First, signaling pathways function at the protein level, so measured gene expression levels from microarrays at most can provide a proxy to the protein activity levels. Second, each cell may behave differently from other cells due to complex interactions among many proteins, some substantially. Therefore, population level data collected by microarrays can mask individual cell differences, making it difficult to infer underlying pathways. In contrast, single cell level protein activity data offer much richer information for pathway inference. Flow cytometry is a powerful fluorescence-based technology that can make rapid, sensitive, and quantitative measurements of multiple proteins for thousands of individual cells. It can measure both a specific protein's expression level and protein modification states such as phosphorylation. Therefore, phospho-protein responses to environmental stimulations can be monitored at the single cell level for thousands of cells very efficiently, and this technology has been employed to infer signaling pathways through gathering activity levels of multiple proteins under different stimulatory or inhibitory conditions. We focus on the analysis of single cell flow cytometry data in this article. Several methods have been applied for network inference based on genomics data, including Bayesian Networks (BNs), Markov Networks (MNs, also called Markov random fields), and Dependency Networks (DNs). Common to all these methods, each protein (or gene) is represented by a node and a dependency between two proteins is represented by an edge in the network. More formally, we define a graph   = (V, E) with its nodes V = {1, ..., P} and an edge set E. We use Xi to refer to the value of the ith node, that is, the expression level of the ith protein. The methods differ in how the edges are inferred from the observed data. In BN, the network is a directed acyclic graph where the state of each node only depends on its immediate ancestors. This structure imposes Markovian dependency among all the nodes stating that each variable is conditionally independent of its nondescendants given its parent variables. So the joint likelihood for all the nodes, that is, proteins, can be factored into a product of conditional probabilities. BNs pose significant computational challenges to learn the network structure because the model space to be explored is super-exponential in the number of genes to be studied. More recently, proposed a method to reduce the bias in the fast mixing algorithm proposed by to sample the BN structures from the posterior distribution. MNs are undirected graphical models and are similar to BNs in representation of dependencies: each random variable is conditionally independent of all other variables given its neighbors. Gaussian Graphical Models (GGMs), a subclass of MNs, assume a multivariate normal distribution as the joint distribution of random variables. The existence of an undirected edge in a GGM is implied by the nonzero partial correlation coefficient derived from the precision matrix. Some studies have found that BNs outperform GGMs in inferring networks based on interventional data where the biological system is perturbed through designed experiments, but GGMs may perform better for observational data, for example,. DNs aim to reduce computational burden where a large number of genes are modeled by building a collection of conditional distributions separately. DNs define the conditional distributions {p(Xi X-i)} separately for each Xi. When we focus on sparse normal models, DNs define a set of P separate conditional linear regression models in which Xi is regressed on a small selected subset of predictor variables, which are determined separately. Because a statistical association between two variables only implies association not causation, standard DN and GGM approaches cannot be used to infer causal networks, a goal in signaling pathway analysis. In this article we develop a Bayesian hierarchical modeling approach based on DNs to address this limitation. This is achieved through appropriate intervention experiments to dissect directional influences. To accommodate varying relationships among proteins under different experimental conditions, we allow a different set of regression models for each condition. At the same time, the hierarchical framework imposes similar functional forms across conditions to borrow information from different experiments. As for causal inference, the basic idea is that for any protein i, its regulators exert similar effects if it is not intervened, and would have no effect when i is controlled. In contrast to standard regression models where the predictors are assumed to be error-free, our model allows measurement errors in predictor variables. A large part of the difficulty in the standard BN computation is due to the requirement that the network be acyclic. Our approach is not guaranteed to give acyclic networks. However, in terms of sensitivity and specificity to detect true edges, our method is competitive with the best methods that impose the acyclic graph assumption. This is illustrated by our results in the example of. The paper is organized as follows. In Section 2 we describe two hierarchical models (a general hierarchical model where no constraints are imposed and a restricted hierarchical model where a symmetry constraint is imposed) and the methods for statistical inference of the network. For comparison, we also describe a nonhierarchical model where all the experiments are pooled together for analysis. Then we investigate the performance of these methods on simulated data in Section 3. In Section 4 we apply these methods to data from a study of the signaling networks of human primary naive CD4+ cells. We finish the paper with discussions in Section 5. 2. Methods. The primary goal of our statistical model is to infer causal influences among proteins from interventional data. In this section we describe three models: a hierarchical model (HM), a restricted hierarchical model (RHM), and a nonhierarchical model (NHM), that can be used to infer the relationships among proteins. We also discuss statistical methods to infer causal networks in this section. 2.1. Hierarchical model (HM). First we discuss a Bayesian hierarchical model to infer the relationships among proteins both at the overall level across all experiments and under individual experimental conditions. Our model incorporates both measurement errors and the intrinsic noises due to the biological process and unmodeled biological variations. Let P denote the number of proteins, K denote the number of experimental conditions, and Nk denote the number of samples (individual cells) under the kth condition, k = 1, 2, ..., K. We further let xink denote the true activity level of the ith protein in the nth cell under the kth experimental condition, and xink the measured value of its activity, where  , and the measurement error   is a normal random variable with mean 0 and standard deviation sigmaM, that is,  . Our model assumes that there exists a linear relationship among the activity levels of proteins. That is, for each protein i = 1, 2, ..., P and for each condition k = 1, 2, ..., K,.   where   is the intrinsic noise and has a normal distribution  .2 We assume that the error terms { } are independent, and are independent of the measurement errors { }. In equation (1),   if there is no linear relationship between the activity levels of proteins i and j under the kth experimental condition. A nonzero value of   implies the existence of a linear relationship (but not necessarily a causal effect). To correctly infer the network among these proteins, we need to first find, for each protein, the subset of proteins that are linearly associated with its expression level, which is implied by the set of nonzero coefficients in (1). The linear relationship among the true expression values xink implies that the observed values are also linearly related:  . Comparing (1) and (2), we can see that correctly inferring the relationship in the network depends on the correct inference of the set of nonzero coefficients in (2). For each protein, we utilize indicator variables zij = 0/1 to denote the relationship between proteins i and j such that zij = 1 if and only if the coefficient of the jth protein in the regression model for the ith protein is nonzero. The values of zij may differ under different experimental conditions. For example, if protein j regulates protein i, zij = 1 when i is not controlled and the association strength between the two proteins should be similar under such conditions. However, zij = 0 when i is controlled because the relation between Xi and Xj is destroyed. Therefore, it is natural to use a hierarchical structure to formalize this thinking. We use   to denote the probability that zij = 1, that is, protein j is related to protein i under the kth experimental condition. The prior we take for the regression coefficient   is a mixture of two distributions. One is a point mass at zero, indicating that the jth protein is not linearly related to the ith protein under the kth condition. The other is a normal distribution for nonzero effects, with weight  . Specifically, the prior for the slope coefficient   is.   where delta0 indicates a point-mass at zero, and   is the probability that  . When  , the prior for   is   with a common mean and a common standard deviation across different experimental conditions. Under this setup, information is shared for coefficients   across different conditions. Similarly, we borrow information for   across different experimental conditions by applying a beta distribution as a prior for   with a common mean wij and a common variance wij(1 - wij)/(vij + 1):  . So   measures the probability that there is an association between proteins i and j under the kth experimental condition, and wij measures the overall-level probability that the two proteins are associated. To complete the model, we specify a beta distribution Beta(beta1, beta2) for wij, a normal distribution N(a(i), tau(i)) for alphaij and gamma distributions: G(gamma1, gamma2), G(gamma3, gamma4) and G(gamma5, gamma6) for   and (sigmaM)-2, as their respective prior distributions. In the simulation studies, we take gammai = betaj = 1 for i = 1, 2 and j = 1, 2, ..., 6, a(i) = 0 and tau(i) = 1000 for i = 1, ..., P. In real data analysis, we vary the hyperparameter values to study the sensitivity of the inference results to these values. We note that the posterior distribution is proper since we take proper priors for all the parameters. One attractive feature of this model is that when   is integrated out, the marginal distribution of   is independent of vij:  . Given   and wij, when vij is specified, the posterior distribution of   is. Hence, we can first sample the posterior distributions of   and wij, and then sample   according to equation (6). Under this model, the inference of the causal network consists of two steps. First, based on the posterior means w(i,j) of the overall-level probability 0.5 x (wij + wji), we infer whether there is an association between proteins i and j with a certain threshold u1. Second, for each pair of proteins (i, j) that are inferred to be associated, we determine their regulatory direction based on the experiment-level probabilities   to infer the causal network. The underlying assumption of our inference is that for a pair of proteins (i and j) that has a regulatory relation, say, i regulates j (i   j), controlling (inhibiting or activating) over protein j affects the activity of j but not i, resulting in much reduced or lack of association between i and j; controlling over protein i affects the activity of i and hence j, keeping the association between them. The posterior distributions of   given   and wij, with vij prespecified, is given in equation (6). To better reflect the changes of   for different k, we use vij   0.1 in our analysis, because larger values of vij (e.g., 10) are not able to reveal the changes in  , as the parameters in (6) are dominated by vij when vij is large, and   plays a smaller role in (6). To put this into more concrete terms, we consider wij = 0.9 as an example, which gives a strong support for the association between proteins i and j. The difference between the distributions Beta(0.9 x 10 + 1, 0.1 x 10) and Beta(0.9x10, 0.1x10+1) when vij = 10 is much less than that between Beta(0.9x0.1+1, 0.1x0.1) and Beta(0.9x0.1, 0.1x0.1+1) when vij = 0.1. To determine the directions of edges, we calculate the posterior means   of   for all k and (i, j) pairs. For each pair (i, j), if all the values in a stream (e.g.,  ) are small (less than a threshold u2 > 0, so the signal in this stream is weak compared to noises), then we ignore this stream and infer the causal relations only based on the other one . The inference is based on checking whether   under specific conditions decreases greatly compared to the highest value. Let Si,j = {k : k   {i, j}} denote the set of conditions under which i or j is perturbed, and  Si,j  be its cardinality. We propose the following four criteria to determine the causal relationship between an associated protein pair (i, j):. Case 1:  Si,j  = 1, that is, i or j is only perturbed in one condition. Without loss of generality, we suppose that protein i is controlled under condition k' (Si,j = {k'}). If   for a threshold u3 > 0, then from stream   we infer j   i. Otherwise, we infer i   j. Similarly, we make an inference from the stream  . If the directions inferred from both streams are the same, say, i   j, we infer that direction as the direction of the edge between i and j: i   j. If the directions from both streams are different, we say that the direction of the edge is undetermined. Taking the conditions in Table 1 for the network in Figure 1 as an example, pairs (1, 2), (1, 8), (2, 6) (3, 4), (4, 5), (6, 8), (8, 10), and (8, 11) belong to Case 1. Case 2:  Si,j  > 1 and for all k   Si,j, the same protein, say, i, is controlled. For each stream, for example,  , if   for all k'   Si,j, then we infer j   i; if   for all k'   Si,j, then we infer i   j; otherwise, we do not infer a direction from this stream. If both streams lead to a directional inference and the directions are the same (Figure 4, top panel), or if only one stream provides a directional inference, then we infer the direction of the edge. Otherwise, the direction is undetermined. For the conditions in Table 1, pairs (1, 9), (3, 9), (5, 7) (6, 7), (9, 10), (9, 11) belong to Case 2. Case 3:  Si,j  > 1 and both proteins are controlled in the experiments. Let   denote the set of conditions under which protein i is controlled, and   the set of conditions under which protein j is controlled. For each stream, for example,  , we calculate the differences of   when i or j is controlled:   for each   and  . If   for all   and  , we infer that i   j; if   for all   and  , we infer that j   i; otherwise, the direction is undetermined from this stream. If both streams lead to a directional inference and the directions are the same, or if only one stream provides a directional inference, then we infer the direction of the edge. Otherwise, the direction is undetermined. For the conditions in Table 1, pairs (2, 9), (4, 9), (7, 8) (8, 9) belong to Case 3. Case 4:  Si,j  = 0, that is, no perturbation is conducted on either protein. In this case, we cannot infer the causal relation. The choices of the thresholds u1, u2, and u3 will be discussed in simulation studies. Generally speaking, the two steps involved in causal network inference are based on the posterior distributions of wij and  . The overall-level probability wij measures the strength of the linear relationship between two proteins across all the conditions. Based on wij, we infer the set of proteins that are related from which we determine an undirected graph. The changes in the experiment-level probabilities   offer insights on the directions of causal regulations. 2.2. Restricted hierarchical model (RHM). The wij and   in (3), (4), and (6) denote the probability that protein j is included in the linear model to predict the activity level of protein i across all the experiments and under the kth specific condition, respectively. In this framework, we may impose the constraint that wji = wij and   for each k, that is, the existence of a linear relationship between proteins i and j is independent of which variable is the predictor and which is the response variable. We can infer the posterior distributions of wij and   under this constraint, and call this model a restricted hierarchical model (RHM). Based on the posterior means of wij, we can infer whether proteins i and j are associated with each other by setting up an appropriate threshold  . For each associated pair, we can infer the causal relationship according to the changes in  . The choice of the threshold will be illustrated in Section 3.1 and Supplementary Material S3 details the criteria in determining the causal relations for the associated pairs of proteins. Different from HM, we must prespecify vij in RHM to sample from the posterior distributions of wij and  . We will show how different values of vij affect the network inference in the following discussion. 2.3. Nonhierarchical model (NHM). To demonstrate the usefulness of the hierarchical model approach, we also consider a nonhierarchical model (NHM) as a reference model for comparisons. The NHM assumes a linear model among the activity levels of proteins and incorporates both measurement errors and intrinsic noises as in equation (2). The main difference is that this NHM assumes identical regression coefficients across different experimental conditions:   where the intrinsic noise   follows the normal distribution  , the measurement error   follows the normal distribution N(0, (sigmaM)2), and they are assumed to be independent. As in HM, we also apply mixture distributions as priors for the coefficients alphaij:  . The posterior distributions of wij provide information about whether proteins i and j are associated. However, it is impossible to make causal inference from this model. For all three models, we use MCMC methods to sample the posterior distributions. Supplementary Material S2 provides details of the MCMC updates for HM. The MCMC updates for RHM and NHM are similar and not shown in this paper. 3. Simulation study. We first apply our methods to simulated data to illustrate how to infer the causal network from the posterior distributions of the overall-level probabilities wij and the experiment-level probabilities  , for both HM and RHM. We also study how the inference differs between these two methods and for different choices of vij. We then study the performance of our methods on simulated data with heavy tail distributed intrinsic noises. We simulate data based on the network shown in Figure 1, which is adapted from by correcting one reversed edge and including three missed edges. From Figure 1, we can derive the parent set for each node (protein). For any protein i, we first generate the association strength alphaij from the uniform distribution over the interval [0.5, 2], and randomly assign the sign of alphaij. Given the activities of its parents, we simulate the activity xi of protein i from the normal distribution:  , where the sum extends over all parents of protein i. Thus, we get the empirical distribution of xi when protein i is not intervened. Let xi denote the observed expression level of protein i, then xi is simulated from N(xi, (sigmaM)2). We simulate the interventional data as follows. For an intervention experiment, if the ith protein is inhibited, we sample xi from the left tail of its empirical distribution obtained when protein i is not perturbed, beyond the 5th percentile. If the ith node is stimulated, we sample xi from the right tail of the empirical distribution, beyond the 95th percentile. We simulate a total of nine stimulatory or inhibitory interventional conditions, as summarized in Table 1. Under each perturbation condition, we simulate expression levels for each of the 11 proteins for 600 individual cells. We consider two cases: (1) constant intrinsic variances   and (2) variable intrinsic variances with  , where IG(2, 1) represents the inverse gamma distribution with mean 1 and variance  . Finally, we simulate data where the intrinsic noises are sampled from a heavy tail distribution: t(1), which represent a central t distribution with one degree of freedom. 3.1. Constant intrinsic variance  . 3.1.1. Inference from HM. Based on the simulated data, we obtain samples for both wij and wji from their posterior distributions under HM. To infer whether an association exists between proteins i and j, we obtain the posterior means w(i,j) of the average of the probability that each is included in the regression model of the other: (wij + wji)/2. Higher values of w(i,j) imply stronger evidence of association between the two proteins. Figure 2 shows the posterior means w(i,j), from one MCMC run, for each pair (i, j) (i < j), in the ascending order of w(i,j). Large solid circles represent true associations, and small empty ones represent false ones. We see that the true associations dominate the higher values of w(i,j). To infer the pair of proteins that are associated, we need to set a threshold u1 on the posterior means w(i,j) so that those above the threshold are inferred to be associated. The permutation study3 offers an over-liberal threshold (<0.1), based on which we get over 40 associations with false positive rate >=0.5. Noting the jumps in the plot of w(i,j), we propose to choose the threshold where a big jump occurs. Setting the threshold u1 as any value between 0.2 and 0.4 and choosing the pairs with w(i,j) > u1, we get 22 associations with 2 false positives. When we have multiple MCMC runs, which lead to multiple plots of w(i,j), we can combine the inferences from them. Figure S1 in the Supplementary Material draws the plots of w(i,j) from four additional MCMC runs. They show the same features as seen in Figure 2 that true associations tend to have high w(i,j) values and jumps exist in these plots. These five MCMC runs lead to quite similar results: from four of them we get 22 associations with 2 false positives, and from a fifth run we get 21 associations with 2 false positives and 1 missing association, when we choose u1 between 0.3 and 0.4. Let uf be the relative frequency that each association is selected. When u1   (0.3, 0.4) and uf >= 4/5, we get 22 associations with 2 false positives (Figure 3). For the pairs of proteins that are inferred to be associated, we then infer their causal directions based on the criteria listed in Section 2.1. To better illustrate the criteria, we give two examples in Figure 4. The top panel draws the boxplots of the samples from the posterior distributions of   and  . Both show that the experimental-level probabilities greatly decreased under conditions 3 and 7 where protein 7 (Akt) is inhibited (here   and   for k' = 3, 7). So we infer the direction 5   7 (i.e., PIP3   Akt). The bottom panel tells a different story. The posterior means of   when k = 3 or 7 are much smaller than others , indicating the causal relation 6   7, but   keeps the same level under all conditions , indicating the causal relation 7   6. The contradictory results from   and   lead to the failure in determining the causal relationship between proteins 6 (Erk) and 7. Taking u2 = 0.1 and u3   (0.3, 0.5), we infer a causal network as shown in Figure 3, which contains 14 true directed edges, 5 edges whose directions are undetermined, 1 reversed edge, and 2 false edges. When we have multiple MCMC runs, we infer the causal relation of each edge based on the majority vote of the directions inferred from each MCMC run. In fact, these five runs lead to almost identical causal inference for the common associations [based on u1   (0.3, 0.4)] when we take u2 = 0.1 and u3   (0.3, 0.5). The choices of u2 and u3 are affected by the value of  . Choosing   ensures that most   are either above 0.9 or below 0.1. The streams with   for all k contain too weak a signal to provide sufficient information for causal inference. So we take u2 = 0.1. The small value of   also leads to a great difference in   for different experiments when protein i or j is intervened. In this simulation study, intervention of the child node for one edge leads to a decrease of at least 0.5 in  . Any value of u3 in (0.3, 0.5) leads to the same directional inference for the inferred associations. 3.1.2. Inference from RHM. RHM requires that wij = wji and   for all i, j, and k. This restriction aims at avoiding the nonconsistent directional inferences based on   and   separately as HM does. We plot the posterior means w(i,j) of wij in Figure 5 where we take vij = 0.1. Similar to Figures 2 and S1, true associations tend to have higher values of w(ij). Setting  , we infer 22 associations with 2 false positives. Applying the criteria listed in Supplementary Material S3 ], we infer the causal network as shown in Figure 5. Compared to the network in Figure 3, RHM leads to a network with 16 true directed edges, 3 edges whose directions are undetermined, 1 reversed edge, and 2 false edges when we take   and u3   (0.3, 0.5). If we increase the threshold   to a value where there is a big jump, for example, 0.3, we will miss 1 true directed edge. When a bigger value vij = 10 is applied, the differences of the posterior means w(i,j) of wij become much smaller between the true and false associations (Figure S2 in the Supplementary Material ]). This together with the fact that bigger values of vij lead to smaller changes in experimental level probabilities results in our conclusion that a small vij is preferred for causal network inference. 3.1.3. Inference from NHM. Ignoring the effect of perturbations on signaling pathway, NHM assumes a common coefficient alphaij in the linear regression models across all experimental conditions. From this model, we can only infer whether there is an association between two proteins. Similar to the inferred posterior means from HM, w(i,j) from NHM also tend to take higher values for true associations (Figure 6), but with two differences. First, the range of w(i,j) from NHM is smaller. In other words, compared to HM, NHM leads to smaller values of the biggest w(i,j), and larger values of the smallest w(i,j). So the support for true associations and the evidence against false associations are weaker. Second, the dominance of high values of true associations is not as strong as that from HM. More false associations take higher values of w(i,j) than the hierarchical inference. If we take 0.6 as a threshold, we infer 23 associations with 15 true and 8 false. Taking 0.45 as the threshold, we recover all the true associations, but 27 false ones are also inferred. More importantly, we cannot determine the directions of associations from NHM because perturbation information is not utilized in this model. 3.2. Variable intrinsic variances  . We then consider the case where variances of intrinsic noises vary for different proteins. In this case, both HM and RHM with vij = 0.1 clearly separate the true associations from the false ones in the plot of the posterior means w(i,j) (Figure 7). The causal networks inferred from both models are the same, with 18 correctly inferred true edges, 1 reversed edge, and 1 edge whose direction is undetermined [u1   (0.2, 0.7), u2 = 0.1, and u3 = 0.3]. As in Section 3.1.2, RHM with a bigger value vij = 10 leads to association inference with bigger false positive rate and smaller changes in experimental level probabilities when a child node is perturbed (Figure S2 in the Supplementary Material ]). NHM is not applied here and thereafter since it does not provide causal relations. 3.3. Heavy tail distribution for intrinsic noise. Considering the possibility of nonnormality for real biological processes, we simulate data where the expression levels of proteins have heavy tail distribution. This is realized by simulating   for each protein i under each experimental condition k. We reuse the parameter settings in Sections 3.1 and 3.2 so that the performance of our methods on the normal and nonnormal cases can be easily compared. We summarize the network inference results in Table 2. Due to the model misspecification when we use HM to analyze these heavy tail distributed data, we infer networks with more false positive and false negative edges. Therefore, our current model needs to be extended to analyze heavy tail data. 4. Case study. The Mitogen-Activated Protein Kinase (MAPK) pathways transduce a large variety of external signals, leading to a wide range of cellular responses such as growth, differentiation, inflammation, and apoptosis. External stimuli are sensed by cell surface markers, then travel through a cascade of protein modifications of signaling proteins, and eventually lead to changes in nuclear transcription. Single cell interventional data of 11 well-studied proteins from the MAPK pathways were originally generated by using the intracellular multicolor flow cytometry technique. This pathway was perturbed by 9 different stimuli, each targeting a different protein in the selected pathway (Figure 1 and Table 1). applied Bayesian network analysis to infer the causal protein-signaling network. Correcting the bias in the commonly used algorithm proposed by reanalyzed this data set through sampling BN structures from the correct posterior distribution. Both studies used the discretized data where the protein expression levels were grouped into three levels: "low," "middle," and "high." The inhibited molecules were set at "low" values, and activated molecules were set to level "high." We apply our method to this data and compare the results with those from. We infer the networks using HM and RHM with vij = 0.1 and vij = 10. Each analysis has five MCMC runs. Figure 8 shows the inferred posterior means w(i,j) in one MCMC run (more can be found in Supplementary Figures S4 ~ S6), and the inferred networks from five MCMC runs, from each method. We use the same symbols as in simulation studies to indicate true or false inferences, where the "true" network is taken to be the network in Figure 3 of, which is the current understanding of this pathway. Compared to HM, RHMs lead to fewer true associations with high values of w(i,j) (vij = 10) or smaller gaps of w(i,j) between most true and false associations (vij = 0.1). Taking the threshold u1 = 0.2 and requiring uf >= 0.6 in five runs of HM, we get 21 associations, with 5 missing edges and 6 false positives. Requiring   and uf >= 0.6 in RHM with vij = 0.1, we get 19 associations, with 5 missing edges and 4 false ones. The threshold 0.11 exceeds the value (0.1) from the permutation study by only a small amount, implying that RHM offers weaker support to true associations than HM. Setting   and uf >= 0.6 in RHM with vij = 10, we only get 14 associations, with 8 missing and 2 false associations. From RHM, we can only correctly infer the directions of five or six edges. The causal relations for most inferred associations can not be determined. But HM leads to a better result: 9 true directed edges, 1 direction-undetermined, 4 reversed, 6 missed, and 4 false edges, under the thresholds u1 = 0.2, u2 = 0.1, u3 = 0.3, and uf >= 0.8 (Figure 8 and Table 3). This inferred network is comparable with that from, which contains 9 true directed edges, 3 reversed, 8 missed, and 6 false edges. The Hamming distances of these two networks to Figure 1 are 15 and 17, respectively. These results are summarized in Table 3. In MCMC analysis, we take betai = gammaj = 1 for i = 1, 2 and j = 1, ..., 6. To check the sensitivity of HM, we also consider other values: gammai = 0.1 or 100, and betaj = 0.1 or 0.0001. Taking betaj = 0.1, we get a network (not shown) with 9 true directed edges, 1 direction-undetermined, 4 reversed, 6 missed, and 6 false associations. Other values of the hyperparameters result in 1~3 fewer true associations, and at least 3 fewer true directed edges. All these results are based on 5,500,000 iterations of MCMC updates in each run, which take about 20 hours on a node with an Intel(R) Xeon(R) 3 GHz CPU and a 16G memory. 5. Discussion. We have proposed hierarchical statistical methods to infer a signaling pathway from single cell data collected from a set of perturbation experiments. The advantage of this method is that it provides a more explicit framework to relate the activity levels of different proteins. In our models, we assume that the activity level of each protein is linearly associated with a small subset of other proteins under each condition. Using a Bayesian hierarchical structure, we model the existence of an association between two proteins both at the overall level and at the experimental level. The overall-level probabilities measure the strength of associations between any two proteins across all experiments. The experimental-level probabilities reflect the changes of associations between proteins under different conditions. Our inferential procedure consists of two steps. First we infer the existence of an association between any pair of proteins based on the overall-level probabilities. Then for those pairs of proteins inferred to be associated, we infer the directions of the causal relations based on the changes in the experimental level probabilities. The basic rationale in our causal inference is that for two associated proteins, controlling over the target molecule destroys the association, while perturbing the regulatory molecule does not. We consider hierarchical models with (RHM) and without (HM) the restriction that wij = wji and   for each k. For RHM, we have to specify the hyperparameter vij prior to MCMC analysis. We have considered the inference results when the value of vij is set at 0.1 and 10. Higher values of vij lead to higher ranges of the inferred overall-level and experimental-level probabilities, and smaller changes in experimental-level probabilities. In HM, the experimental-level probabilities can be integrated out, so the posterior inference of other parameters is independent of vij. Hence, the choice of associations, which is based on the overall-level probabilities, is independent of vij. We only need to specify vij in the causal inference. To better reflect the changes of the experimental-level probabilities, we suggest smaller values for vij, for example, vij = 0.1. Both HM and RHM perform well in simulation studies. We need to choose thresholds to infer the causal network: u1 for association inference and u2 and u3 for causal directional inference. Noting the jumps in the plots of w(ij), we propose to choose the threshold u1 where there are great differences in sorted w(ij). This is easily determined when variations in the data are well captured by the proposed hierarchical models (e.g., Figures 2 and 7). If there are no great differences in the sorted overall-level probabilities (e.g., Figure 8), one may decide the number of edges to be included and then choose the top ones. Threshold u2, which is taken as 0.1 in our study, can be chosen based on the experimental-level probabilities of those unassociated pairs of proteins. Threshold u3 is closely related to vij, which measures the variability in   in that a smaller vij leads to greater variabilities in   between the experiments when the target protein is and is not intervened. When vij = 0.1, a difference of 0.3 in   is enough to show the effect of intervening the target protein. Compared to the nonhierarchical model, hierarchical models have at least two advantages. First, the hierarchical structure allows information borrowing across different experiments while allowing for differences among experiments, leading to a more clear-cut inference on whether two proteins are related. Second, this modeling framework allows us to infer causal relationships between proteins from the presence and absence of the association across different perturbation conditions. Overall, our proposed hierarchical modeling provides a general framework for inferring networks from high-throughout data. There are several possible ways of extending this model. In Supplementary Material S1 we modify HM by incorporating varying variances of intrinsic noises under different experimental conditions. The modified model does not outperform HM in our simulation study. It is interesting to investigate when the varying variances of intrinsic noises are not ignorable and incorporating them improves the network inference. We also find in our simulation study that applying our methods to data where intrinsic noises are sampled from heavy tail distributions results in power loss in pathway inference. Therefore, there is a need to extend this hierarchical structure to model nonnormal data. Supplementary Material. Supported in part by National Science Foundation Grant DMS-07-14817, National Heart, Lung, and Blood Institute Contract N01 HV28186, National Institute of Drug Abuse Grant P30 DA018343, National Institute of General Medical Sciences Grant R01 GM59507, Yale University Biomedical High Performance Computing Center and NIH Grant RR19895, which funded the instrumentation.                          . Here a constant variance   is assumed for the intrinsic noises of a particular protein. We can relax this assumption and allow varying variances   for intrinsic noises under different experimental conditions. This extended model and simulation results are described in Supplementary Material S1.                          . We permute the observations for each protein and then analyze the permuted data with HM. The obtained posterior means w(i,j) are less than 0.1 for all (i, j) pairs.                          . Sparse graphical models for exploring gene expression data. Learning causal Bayesian network structures from experimental data. Being Bayesian about network structure. Dependency networks for inference, collaborative filtering, and data visulization. The history and future of the fluorescence activated cell sorter and flow cytometry: A view from Stanford. Revealing signaling pathway deregulation by using gene expression signatures and regulatory motif analysis. Supplementary material for "Bayesian hierarchical modeling for signaling pathway inference from single cell interventional data. Bayesian network analysis of signaling networks: A primer. Inferring subnetworks from perturbed expression profiles. Simultaneous measurement of multiple active kinase states using polychromatic flow cytometry. Causal protein-signaling networks derived from multiparameter single-cell data. An empirical Bayes approach to inferring large-scale gene association networks. A Markov random field model for network-based analysis of genomic data. A hidden spatial-temporal Markov random field model for network-based analysis of time course gene expression data. Comparative evaluation of reverse engineering gene regulatory networks with relevance networks, graphical Gaussian models and Bayesian networks. Pathway adapted from by including three missed edges and correcting one reversed edge. Nodes represent proteins, and directed edges represent signal transduction. Posterior means w(i,j) of (wij + wji)/2, sorted in increasing order, from one MCMC run of the HM on the simulated data with constant intrinsic variances. Large solid and small empty circles represent true and false associations, respectively. Networks inferred by choosing associations with u1   (0.3, 0.4), u2 = 0.1, u3   (0.3, 0.5), and uf >= 0.8 in five MCMC runs of the HM on the simulated data with constant  . Solid arrowed lines represent correctly inferred true edges, dashed thick lines with labels "u" represent edges whose directions cannot be determined from the simulations, dashed arrowed thin lines with labels "r" represent reversed edges, and dotted lines with labels "+" represent false positive edges. Boxplots of the samples from the posterior distributions of   and   (top panel),   and   (bottom panel) when vij   0.1 for all i and j. This is from one MCMC run of the HM on the simulated data with constant  . Inference from RHM with vij = 0.1 on the simulated data with constant  . Left: posterior means w(i,j) of wij, sorted in increasing order. Right: inferred networks with  , u3   (0.3, 0.5). Solid arrowed lines represent correctly inferred true edges, dashed thick lines with labels "u" represent edges whose directions cannot be determined from the simulations, dashed arrowed thin lines with labels "r" represent reversed edges, and dotted lines with labels "+" represent false positive edges. Posterior means w(i,j) of (wij + wji)/2, sorted in increasing order, from one MCMC run of NHM. Small empty and large solid circles represent the false and true associations, respectively. Inference results for the simulated data with variable intrinsic variances. Upper panel: posterior means w(i,j) of wij, sorted in increasing order from HM (left) and RHM with vij = 0.1 (right). Lower panel: inferred networks from both models with u1   (0.2, 0.7) and u3 = 0.3. Solid arrowed lines represent correctly inferred true edges, dashed thick lines with labels "u" represent edges whose directions cannot be determined from the simulations, and dashed arrowed thin lines with labels "r" represent reversed edges. Inference results for the real data. From top to bottom: HM, RHM with vij = 0.1, and RHM with vij = 10. In networks, solid arrowed lines represent correctly inferred true edges, dashed thick lines with labels "u" represent edges whose directions cannot be determined from the simulations, dashed arrowed thin lines with labels "r" represent reversed edges, dotted arrowed thick lines represent missing edges, and dotted thin lines with labels "+" represent false positive edges. A summary of the nine experimental conditions for the data in .                                                Stimulus              Effect                                                       1              CD3, CD28              general perturbation                                     2              ICAM2              general perturbation                                     3              Akt-inhibitor              Inhibits Akt                                     4              G0076              Inhibits Pkc                                     5              Psi              Inhibits Pip2                                     6              U0126              Inhibits Mek                                     7              Ly              Inhibits Akt                                     8              PMA              Activates Pkc                                     9              beta2cAMP              Activates Pka                             . All are based on u1 = 0.4 and u3 = 0.3. The hamming distance is the minimum number of simple operations needed to go from the inferred graph to the true graph. Here simple operations include adding or removing an edge, and adding, removing, or changing the direction of an edge. Data-1: simulated data in Section 3.1 with constant intrinsic variances. Data-2: simulated data in Section 3.2 with varying intrinsic variances. Data-1t: simulated data with parameter settings in Data-1 and intrinsic noises sampled from t(1). Data-2t: simulated data with parameter settings in Data-2 and intrinsic noises sampled from t(1).                     .                 . Summary of pathway inference in simulation study.                                  Data              Methods              True              Undetermined              Reversed              Missing              False              Hamming distance                                                       Data-1              HM              14              5              1              0              2              8                                                   RHM              16              3              0              1              2              6                                     Data-2              HM              18              1              1              0              0              2                                                   RHM              18              1              1              0              0              2                                     Data-1t              HM              9              4              1              6              4              15                                     Data-2t              HM              14              1              0              5              6              12                             . Here mHM denotes the modified model described in Supplementary Material S1 which models the varying variances of intrinsic noises.                     .                 . Summary of the inferred networks applying different methods to the real data.                                                True              Undetermined              Reversed              Missing              False              Hamming distance                                                       HM              9              1              4              6              4              15                                     RHM vij = 0.1              6              8              1              5              4              18                                     RHM vij = 10              5              5              2              8              2              17                                     mHM              6              5              1              8              4              18                                     BN              9              0              3              8              6              17                             .